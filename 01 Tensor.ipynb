{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38864bittorchx86condad5999c8bd9c4485a99dbef96feddd77f",
   "display_name": "Python 3.8.8 64-bit ('torch_x86': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## 1D with numpy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0.,1.,2.,3.,4.,5.,6.])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rank of t :  1\nShape of t :  (7,)\n"
     ]
    }
   ],
   "source": [
    "print('Rank of t : ', t.ndim)\n",
    "print('Shape of t : ', t.shape)"
   ]
  },
  {
   "source": [
    ".ndim : numpy의 차원을 출력\n",
    ".shape : numpy의 크기를 출력  (7,)은 (1,7)을 의미함."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'t[0] t[1] t[-1] = 0.0 1.0 6.0'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "f't[0] t[1] t[-1] = {t[0]} {t[1]} {t[-1]}'"
   ]
  },
  {
   "source": [
    "numpy에서는 범위를 지정하여 원소를 가져올 수 있습니다. 이를 slicing이라고 합니다. \"배열\\[시작번호 : 끝번호\\]\"를 지정하면 끝번호를 제외한 범위의 원소를 가져옵니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2. 3. 4.]\n[4. 5.]\n"
     ]
    }
   ],
   "source": [
    "print(t[2:5])\n",
    "print(t[4:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t[:2] t[3:] = [0. 1.] [3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "print(f't[:2] t[3:] = {t[:2]} {t[3:]}')"
   ]
  },
  {
   "source": [
    "## 2D with numpy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.  2.  3.]\n [ 2.  4.  6.]\n [ 3.  6.  9.]\n [ 4.  8. 12.]]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([ [(j+1.0)*(i+1.0) for j in range(3)] for i in range(4)])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rank  of t:  2\nShape of t:  (4, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Rank  of t: ', t.ndim)\n",
    "print('Shape of t: ', t.shape)\n"
   ]
  },
  {
   "source": [
    "# 파이토치 텐서\n",
    "\n",
    "파이토치의 텐서는 numpy와 매우 유사하지만 더 좋습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([float(i) for i in range(7)])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\ntorch.Size([7])\ntorch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim())  # 차원\n",
    "print(t.shape)  # 크기\n",
    "print(t.size()) # 크기"
   ]
  },
  {
   "source": [
    "파이토치에서도 슬라이싱을 제공합니다. 사용은 numpy와 동일합니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.) tensor(1.) tensor(6.)\ntensor([2., 3., 4.]) tensor([4., 5.])\ntensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(t[0], t[1], t[-1])\n",
    "print(t[2:5], t[4:-1])\n",
    "print(t[:2],t[3:])"
   ]
  },
  {
   "source": [
    "## 2D with PyTorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n        [ 2.,  4.,  6.],\n        [ 3.,  6.,  9.],\n        [ 4.,  8., 12.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([ [(j+1.0)*(i+1.0) for j in range(3)] for i in range(4)])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\ntorch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim())\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2., 4., 6., 8.])\ntorch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(t[:,1])       # 모든 첫번째 차원에서 두번째 차원의 첫번째 원소만 가져온다.\n",
    "print(t[:,1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 2.],\n        [2., 4.],\n        [3., 6.],\n        [4., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(t[:,:-1])     # 모든 첫번째 차원에서 두번째 차원의 마지막 원소를 제외하고 모두 가져온다."
   ]
  },
  {
   "source": [
    "## 브로드캐스팅\n",
    "\n",
    "두 행렬 A, B가 있습니다. 행렬의 덧셈과 뺄셈을 할 때는 두 행렬 A, B의 크기가 동일해야 된다고 배웠습니다. 하지만 딥러닝에서는 그렇지 못한 경우가 발생합니다. 이때 자동으로 크기를 맞춰서 연산해주는 것이 브로드캐스팅입니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([[2, 2]])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "source": [
    "m1과 m2의 크기는 둘다 (1,2)이므로 문제 없이 덧셈 연산이 가능합니다. 이번에는 크기가 다른 텐서들 간의 연산을 해보겠습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[4., 5.]])\ntorch.Size([1, 2]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1,2]]) # vector\n",
    "m2 = torch.FloatTensor([3])     # scalar\n",
    "print(m1 + m2)\n",
    "print(m1.size() , m2.size())"
   ]
  },
  {
   "source": [
    "결과를 보듯이 m2가 (1,2) 차원으로 변경되어 연산이 수행된 것을 알 수 있습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[4., 5.],\n        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1, 2]])    # 2,1 vector\n",
    "m2 = torch.FloatTensor([[3], [4]])  # 1,2 vector\n",
    "print(m1 + m2)"
   ]
  },
  {
   "source": [
    "두 벡터는 수학적으로 연산이 불가능합니다. 파이토치에서는 두 벡터를 (2,2) 크기로 변경하여 연산되었습니다.\n",
    "\n",
    "```\n",
    "[1, 2]\n",
    "==> [[1, 2],\n",
    "     [1, 2]]\n",
    "[3],[4]\n",
    "==> [[3, 3],\n",
    "     [4, 4]]\n",
    "```\n",
    "\n",
    "브로드캐스팅은 이처럼 크기가 다른 두 텐서를 연산할 수 있다는 점에서 좋지만, 반대로 두 텐서의 크기가 다른지 모른 상태로 어떠한 에러가 없다면 의도치 않은 결과를 낼 것입니다. 그리고 이러한 오류가 어디서 발생했는지 알 수 없을 것 입니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 주요 사용되는 기능\n",
    "\n",
    "## 행렬 곱셈과 곱셈의 차이\n",
    "\n",
    "행렬로 곱셈을 하는 방법은 크게 두가지가 있습니다. 행렬 곱셈과 원소 별 곱셈입니다. 파이토치 텐서의 행렬 곱셈을 보겠습니다. 이를 matmul()을 통해 수행합니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of Matrix 1:  torch.Size([2, 2])\nShape of Matrix 2:  torch.Size([2, 1])\ntensor([[ 5.],\n        [11.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
    "print(m1.matmul(m2)) # 2 x 1"
   ]
  },
  {
   "source": [
    "행렬 곱셈 이외에도 element-wise 곱셈이 있습니다. 이는 동일한 크기의 행렬이 동일한 위치에 있는 원소끼리 곱하는 것을 말합니다. 여기서는 브로드캐스팅이 된후에 element-wise 곱셈이 수행되는 것을 보여줍니다. 이를 * 또는 mat()을 통해 수행합니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of Matrix 1:  torch.Size([2, 2])\nShape of Matrix 2:  torch.Size([2, 1])\ntensor([[1., 2.],\n        [6., 8.]])\ntensor([[1., 2.],\n        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
    "print(m1 * m2) # 2 x 2\n",
    "print(m1.mul(m2))"
   ]
  },
  {
   "source": [
    "m1 행렬의 크기는 (2,2) 였습니다. m2 행렬의 크기는 (2,1)였습니다. 이때 element-wise 곱세을 수행하면, 두 행렬의 크기는 브로드캐스팅된 후에 곱셈이 수행됩니다.\n",
    "\n",
    "```\n",
    "[1]\n",
    "[2]\n",
    "==> [[1, 1],\n",
    "     [2, 2]]\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 평균\n",
    "\n",
    "평균을 구하는 방법은 쉽습니다. .mean()을 사용하여 원소의 평균을 구합니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.5000)\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([1,2])\n",
    "print(t.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2.5000)\ntensor([2., 3.])\ntensor([1.5000, 3.5000]) tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1,2],[3,4]])\n",
    "print(t.mean())\n",
    "print(t.mean(dim=0))\n",
    "print(t.mean(dim=-1), t.mean(dim=1))"
   ]
  },
  {
   "source": [
    "## 덧셈\n",
    "\n",
    "덧셈은 평균 연산과 연산 방법이나 인자가 의미하는 바는 정확히 동일합니다. 다만 평균이 아니라 덧셈을 할 뿐입니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(10.)\ntensor([4., 6.])\ntensor([3., 7.])\ntensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1,2,],[3,4]])\n",
    "print(t.sum())\n",
    "print(t.sum(dim =0))\n",
    "print(t.sum(dim =1))\n",
    "print(t.sum(dim =-1))"
   ]
  },
  {
   "source": [
    "## 최대와 아그맥스(ArgMax)\n",
    "\n",
    "최대는 원소의 최대값을 리턴하고, 아그맥스는(ArgMax)는 최대값을 가진 인덱스를 리턴합니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 2.],\n        [3., 4.]])\ntensor(4.)\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1,2],[3,4]])\n",
    "print(t)\n",
    "print(t.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.return_types.max(\nvalues=tensor([3., 4.]),\nindices=tensor([1, 1]))\ntorch.return_types.max(\nvalues=tensor([2., 4.]),\nindices=tensor([1, 1]))\ntorch.return_types.max(\nvalues=tensor([2., 4.]),\nindices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=0))\n",
    "print(t.max(dim=1))\n",
    "print(t.max(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max : tensor([3., 4.])\narg max : tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"max : {t.max(dim=0)[0]}\")\n",
    "print(f\"arg max : {t.max(dim=0)[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}